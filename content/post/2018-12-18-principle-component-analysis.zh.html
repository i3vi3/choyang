---
title: "主成分分析"
author: yang
date: '2018-12-18'
slug: pricinple-component-analysis
categories:
  - statistics
tags:
  - liner algebra
  - machine learning 
summary: "**ggplot**入门，简要了解**ggplot**的基本构成"
output:
  blogdown::html_page
---



<p>通常高通量数据中含有很多变量，主成分分析是一种数据降维方法，利用正交变换把原始的可能相关的变量转换为一组正交新变量, 提取数据中重要的特征，去除不重要的特征（噪声）。<strong>方差越大，表示的特征信息越多，的选择方差最大的方向，去除方差较小的方向。</strong>。<!--more--></p>
<p>比如微生物组的16S rRNA测序数据，通常每个样品会含有多个OTU（假设有500个）。为了根据OTU丰度对不同分组的样本进行分类，每个OTU的丰度差异都可以在一定程度上反应样品之间的差异，但是不同的OTU之间可能存在着一定的相关性，可能会造成信息的冗余。PCA就是在保持原有变量所包含信息的前提下，减少变量个数进行分析。</p>
<div id="pca---" class="section level4">
<h4>PCA原理 - 奇异值分解</h4>
<p>假设有一组包含<span class="math inline">\(m\)</span>个变量的数据，含有<span class="math inline">\(n\)</span>个样本，构成了一个<span class="math inline">\(m \times n\)</span>的矩阵<span class="math inline">\(A_0\)</span>。用图形表示，<span class="math inline">\(A_0\)</span>是指在<span class="math inline">\(R^m\)</span>空间内的<span class="math inline">\(n\)</span>个点，当<span class="math inline">\(A\)</span>的每一行都减去该行的均值后（数据中心化），<span class="math inline">\(n\)</span>个点通常聚集在一条线或者平面周围（或其他<span class="math inline">\(R^m\)</span>的低维子空间)。</p>
<p>以含两个变量的<span class="math inline">\(n\)</span>个点为例说明PCA的步骤，<span class="math inline">\(A\)</span> 维度为<span class="math inline">\(2 \times n\)</span>。<span class="math inline">\(A\)</span>的每一行减去该行的均值，使<span class="math inline">\(A\)</span>中心化，每一行的均值为0。如图所示，这<span class="math inline">\(n\)</span>个点分布在一条直线周围。由图中也可以直观的看到，主成分方向还表示原始数据在该方向上投影方差最大（分散），与之垂直的投影距离最小。</p>
<p><img src="/post/2018-12-18-principle-component-analysis.zh_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><span class="math inline">\(AA^T\)</span>构成一个<span class="math inline">\(2 \times 2\)</span>的矩阵，其中<span class="math inline">\((AA^T)_{11}\)</span>和<span class="math inline">\((AA^T)_{22}\)</span>是行向量的方差，而<span class="math inline">\((AA^T)_{12}\)</span>和<span class="math inline">\((AA^T)_{21}\)</span>即是两行的协方差。因此，协方差矩阵可表示为</p>
<p><span class="math display">\[
S = \frac {AA^T} {n-1}
\]</span></p>
<p><strong>中心化的原因：保证第一个主分量是方差最大的方向，如果不中心化，那么第一主成分的方向受均值的影响而有一定的倾向性，如下图</strong>。</p>
<p><img src="/post/2018-12-18-principle-component-analysis.zh_files/data_centered.jpg" /></p>
<p>求协方差矩阵的特征值<span class="math inline">\(\sigma_1\)</span>和<span class="math inline">\(\sigma_1\)</span>。根据矩阵奇异值分解公式，中心化后的<span class="math inline">\(A\)</span>可表示为</p>
<p><span class="math display">\[
A = \sqrt{\sigma_1}u_1v_1^T + \sqrt{\sigma_2}u_2v_2^T
\]</span></p>
<p>假设中心化后<span class="math inline">\(A\)</span>为（各行和为0）</p>
<p><span class="math display">\[
A = \left[
    \begin{matrix}
        3 &amp; -4 &amp; 7 &amp; 1 &amp; -4 &amp; -3 \\
        7 &amp; -6 &amp; 8 &amp; -1 &amp; -1 &amp; -7 
    \end{matrix}
\right] \qquad 
S = \frac{AA^T}{5} = \left[
    \begin{matrix}
        20 &amp; 25 \\
        25 &amp; 40 
    \end{matrix}
\right]
\]</span></p>
<p><span class="math inline">\(S\)</span>特征值为57和3，那么<span class="math inline">\(A\)</span>可表示为</p>
<p><span class="math display">\[
A = \sqrt{57}u_1v_1^T + \sqrt{3}u_2v_2^T
\]</span></p>
<p><span class="math inline">\(u_1\)</span>对应于散点图变化最大的方向（直线），<span class="math inline">\(u_2\)</span>垂直于<span class="math inline">\(u_1\)</span>，表征垂直于直线方向上的小幅摆动。</p>
<p>所以，对于多维变量，把原始数据转换到向量<span class="math inline">\(u\)</span>构成的空间内，特征值越大表征在该向量的方向上数据变异越大。通常选择变异量较大的前几个变量用于表征数据。</p>
</div>
<div id="pca" class="section level3">
<h3>PCA数学推导</h3>
<p>以最大投影方差法推导，原始数据的中心点为：</p>
<p><span class="math display">\[
\bar{x} = \frac{\sum_{n=1}^{N}}{N}
\]</span>
原始数据向投影向量<span class="math inline">\(u\)</span>投影之后的方差为：</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{n-1}^{N}(u^Tx_n - u^T\bar{x})^2 = u^TSu
\]</span>
根据梯度优化原则（拉格朗日乘子）:</p>
<p>$$
u^TSu + (1-u^Tu) = 0 \</p>
<p>Su = u
$$
因此，对于多维数据，协方差矩阵<span class="math inline">\(S\)</span>的特征值即为方差最大方向，从公式中也可看出PCA分析需要预先对数据中心化。</p>
<div class="section level4">
<h4>相关系数矩阵</h4>
<p>协方差矩阵是没有消除量纲的表示变量之间关系。当变量的单位对结果有影响的时候。通常选择消除量纲的相关矩阵进行PCA分析。</p>
<p>协方差：</p>
<p><span class="math display">\[
Cov(X, Y) = E((X - E(X))(Y - E(Y)))
\]</span></p>
<p>相关系数：</p>
<p><span class="math display">\[
Cor(X, Y) = \frac {Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
\]</span></p>
<p>所以标准化后的协方差就是相关系数。<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>标准化</p>
<p><span class="math display">\[
X^* = \frac{X - E(X)}{\sqrt{D(x)}}
Y^* = \frac{Y - E(Y)}{\sqrt{D(Y)}}
\]</span></p>
<p><span class="math display">\[
Cov(X^*, Y^*) = \frac {E((X - E(X))(Y - E(Y)))}{\sqrt{D(X)}\sqrt{D(Y)}} = \frac {Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}} = Cor(X, Y)
\]</span></p>
</div>
<div id="r" class="section level4">
<h4>主成分分析R代码实现</h4>
<p>以<code>mtcars</code>数据为例，<strong>PCA</strong>是针对数值数据，所以我们删除<code>mtcars</code>中的分类变量<code>vs</code>和<code>am</code>。</p>
<pre class="r"><code>library(dplyr)
# 首先中心化，因为各变量量纲不同scale = TRUE,
# 主成分是原始变量的线性组合，$rotation表征了线性组合系数，x
# 表示新的坐标值
mtcars_pca &lt;- prcomp(select(mtcars, -c(&quot;vs&quot;, &quot;am&quot;)),
  center = TRUE, scale = TRUE)

summary(mtcars_pca)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4     PC5     PC6
## Standard deviation     2.3782 1.4429 0.71008 0.51481 0.42797 0.35184
## Proportion of Variance 0.6284 0.2313 0.05602 0.02945 0.02035 0.01375
## Cumulative Proportion  0.6284 0.8598 0.91581 0.94525 0.96560 0.97936
##                            PC7    PC8     PC9
## Standard deviation     0.32413 0.2419 0.14896
## Proportion of Variance 0.01167 0.0065 0.00247
## Cumulative Proportion  0.99103 0.9975 1.00000</code></pre>
<p>共9个主成分32个线性相关变量重新组合成9个正交变量，第一个主成分PC1解释了总体数据63%的特征，PC2解释了23%的总体特征，PC1和PC2解释了86%的数据特征，因此仅仅通过前两个主成分就能基本确定样本的位置。</p>
<div id="pca" class="section level5">
<h5>PCA结果可视化</h5>
<p><strong>biplot</strong>显示样本在新的坐标下的位置，同时显示原始的变量值(根据<code>rotation</code>在相应的主成分标出原始值)，向量箭头起始于中心点。</p>
<pre class="r"><code>library(ggbiplot)
ggbiplot(mtcars_pca, labels=rownames(mtcars))</code></pre>
<p><img src="/post/2018-12-18-principle-component-analysis.zh_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>从图中可以看出，变量<code>cyl</code>、<code>disp</code>、<code>wt</code>、<code>hp</code>值较高的样本倾向于位于图中右侧。显示这些变量与哪些样品有关。</p>
</div>
<div class="section level5">
<h5>结果解读</h5>
<p>对车进行分类，观察车出产地情况</p>
<pre class="r"><code>mtcars_country &lt;- c(rep(&quot;Japan&quot;, 3), rep(&quot;US&quot;,4), rep(&quot;Europe&quot;, 7),rep(&quot;US&quot;,3), &quot;Europe&quot;, rep(&quot;Japan&quot;, 3), rep(&quot;US&quot;,4), rep(&quot;Europe&quot;, 3), &quot;US&quot;, rep(&quot;Europe&quot;, 3))

ggbiplot(mtcars_pca,ellipse=TRUE,  labels=rownames(mtcars), groups=mtcars_country)</code></pre>
<p><img src="/post/2018-12-18-principle-component-analysis.zh_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>美国产汽车聚集到右侧，且<code>cyl</code>,<code>disp</code>,<code>wt</code>较大；日本产汽车<code>mpg</code>较大，欧洲产汽车位于中间且相对分散一些。</p>
<p>第三和四主成分情况</p>
<pre class="r"><code>ggbiplot(mtcars_pca,ellipse=TRUE,choices=c(3,4),   labels=rownames(mtcars), groups=mtcars_country)</code></pre>
<p><img src="/post/2018-12-18-principle-component-analysis.zh_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>显然数据没有什么特点，这并不奇怪，因为PC3和PC4仅能表征数据很少的一部分特征。</p>
<p>综上，<code>cyl</code>，<code>disp</code>,<code>wt</code>,<code>mpg</code>可以用于区分美国和日本产汽车，如果需要构建一个汽车产地的模型，这些变量可能非常有用。</p>
</div>
</div>
<div class="section level4">
<h4>参考</h4>
<ul>
<li><a href="https://blog.csdn.net/hit1524468/article/details/60323173">主成分分析（PCA）及Demo最大方差解释和最小平方误差解释</a></li>
<li><a href="https://www.zhihu.com/question/41120789/answer/481966094">如何通俗易懂地讲解什么是 PCA 主成分分析</a></li>
<li><a href="https://www.jianshu.com/p/1518369c2c61">主成分分析（PCA）模型</a></li>
<li><a href="https://www.zhihu.com/question/37069477">数据什么时候需要做中心化和标准化处理</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/pca-analysis-r">Principal Component Analysis in R</a></li>
<li><a href="https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance">PCA on correlation or covariance?</a></li>
<li><a href="https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca">Why do we need to normalize data before principal component analysis (PCA)?</a></li>
<li><a href="https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca">How does centering the data get rid of the intercept in regression and PCA?</a></li>
</ul>
</div>
</div>
